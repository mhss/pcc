Segundo projeto de PCC

Grupo:
Mário Henrique Santos da Silva (mhss)
	Árvore de sufixo
	Maior parte do front-end
	Scripts de teste
Israel Batista Freitas da Silva (ibfs)
	Array de sufixo
	Algoritmos de compressão
	Testes

Implementação

No modo de indexação, o conteúdo do arquivo é lido, e o conjunto de caracteres dele é determinado. Depois é construída a árvore de sufixo ou array de sufixo, dependendo da escolha do usuário. Então, em um arquivo são escritos o alfabeto e o índice do texto. O índice é representado por uma sequência de inteiros e caracteres, cada um escrito com fibonacci encoding. Então são escritos o tamanho e número de linhas do texto. Por último, o próprio texto é comprimido e escrito no arquivo. O algoritmo de compressão pode ser o LZ77 ou LZ78.
No modo de consulta, o arquivo de índice é aberto e são lidos o alfabeto do texto, o índice e o próprio texto, este último sendo descomprimido. O arquivo de índice informa o algoritmo de compressão usado e se o índice é uma árvore ou array de sufixo. Depois o programa percorre o texto pra determinar as posições de fim de cada linha. Então, para cada padrão a ser buscado, o programa encontra suas ocorrências no índice, determina as linhas às quais cada uma pertence e armazena elas num conjunto. Finalmente, cada linha com ocorrência de pelo menos um padrão é impressa, ou só a quantidade delas.

Algoritmos de Indexação
- Árvore de sufixo
O algoritmo usado para a construção foi o mostrado em aula, Ukkonen de 1995, com algumas alterações. Cada nó explícito da árvore é representado por um struct com um map que guarda as transições que saem daquele nó, indexadas pelo primeiro caractere da aresta. Os nós da árvore são guardados num array. Os suffix links são armazenados fora do struct, em um array alocado dinamicamente, pois são necessários somente durante a construção da árvore.
Os nós implícitos da árvore são representados por um struct com um nó explícito e um intervalo da string. Esse mesmo struct, por conveniência, representa as transições que saem de cada nó.
Para ser escrita num arquivo, a árvore é convertida numa sequência de inteiros e caracteres. O primeiro é a quantidade total de nós, e depois dele vêm, em sequência, as serializações de cada nó. A serialização de um nó consiste da quantidade de transações que saem dele, seguida pela sequência dessas transações. Cada transação é representada pelo índice do nó alvo e o intervalo da string que contém o rótulo da aresta. Cada valor é codificado no arquivo utilizando fibonacci encoding.
Para buscar um padrão, o programa procura o nó explícito mais baixo na árvore cujo rótulo tenha como prefixo o padrão buscado. Se ele existir, para cada folha da subárvore dele, o programa pega a altura e com ela determina a posição da ocorrência correspondente.

- Array de sufixo
O algoritmo utilizado para a construção do array de sufixo foi o Manber & Myers. São utilizados 4 arrays: um para o próprio array de sufixo, um para guardar o rank de cada sufixo, um para realizar o counting sort, e outro temporário que funciona como array de sufixo temporário e rank temporário. O rank inicial é dado pelo valor do caractere no Alfabeto.
As buscas são realizadas através do uso de duas buscas binárias: uma encontra o menor índice i no array de sufixo, a outra encontra o maior índice j no array de sufixo, tal qual, o prefixo de tamanho |P| do sufixo é igual ao padrão procurado, dado que P é o padrão procurado, e |P| representa o tamanho do padrão. O intervalo fechado de i à j possui todos os sufixos que possuem o padrão procurado como prefixo. Todos estes sufixos são processados e as linhas em que eles ocorrem são encontradas e reportadas pelo programa. O custo da busca é O(|P| * log(N)), onde N é o tamanho do texto.

Algoritmos de Compressão
- LZ77
O algoritmo LZ77 implementado foi a versão padrão do algoritmo, com tamanho de 1024 para cada array (dictionary e preview), e utilizando-se o algoritmo Aho-Corasick no preview, para encontrar o maior matching no array dictionary, em tempo linear no tamanho dos arrays.

- LZ78
O algoritmo LZ78, no momento de codificar o texto, utiliza uma estrutura de dados chamada Trie que funcionará como o dicionário necessário ao algoritmo. Esta Trie é implementada através da struct LzNode, declarada no mesmo arquivo do LZ78. No momento de descodificar o texto, é utilizado apenas um array de tamanho dinâmico (std::vector) para realização do processo.


Detalhes

Nos argumentos pra consulta, botamos o padrão depois do texto, pra ficar mais fácil fazer testes.
O programa só aceita textos com caracteres ASCII com valor de 0 a 255.
O consumo de memória durante a indexação tende a ser muito grande, chegando a 3GB para arquivos de mais de 25MB.
O programa só aceita padrões de no máximo 1000 caracteres.
Não conseguimos executar o programa corretamente com textos maiores que 35MB.
O custo assintótico de alguns dos algoritmos utilizados é, na verdade, multiplicado pelo log do tamanho do alfabeto utilizado, por causa da utilização frequente da estrutura de dados std::map.

Testes

Os testes da ferramenta foram realizados em uma máquina virtual XUbuntu. A máquina hospedeira possui processador Intel Core i7 de 2.00 GHz, memória de 16GB, com SO Windows 8.1. Já a máquina virtual, possui 8 GB de memória e dois dos 4 núcleos de processamento.
Medimos o tempo gasto para indexar arquivos de texto, bem como o tamanho final dos arquivos gerados em comparação com os originais. Testamos 5 arquivos de texto diferentes, pegando partes de tamanhos entre 5 e 35MB.
Também medimos o tempo médio gasto com consultas de conjuntos de padrões nos índices dos textos. Cada consulta foi realizada três vezes para se obter uma média, e o desempenho foi comparado com o do programa GNU grep 2.16. Para alguns textos geramos padrões aleatoriamente, variando seus tamanhos. Para outros, testamos conjuntos de cadeias presentes no texto. Também testamos conjuntos vazios de padrões, para medir o tempo de leitura do índice e descompressão do texto.

Dados

Os arquivos de texto utilizados nos testes estão disponíveis em http://pizzachili.dcc.uchile.cl/texts.html. Utilizamos até os primeiros 35MB de cada um, exceto o PITCHES, pelo seu formato inconveniente.
Alguns dos padrões utilizados nas consultas foram gerados aleatoriamente em conjuntos de 100, com tamanhos entre 2 e 10 e formados apenas por letras presentes nos arquivos de texto onde eram buscados. Já outros padrões eram cadeias escolhidas aleatoriamente no texto.

Configuração

Primeiro testamos a configuração que considerávamos mais eficiente: árvore de sufixo com o algoritmo de compressão LZ78. Avaliamos a construção do índice, o tempo gasto e o tamanho do arquivo gerado. Também comparamos o desempenho das consultas por padrões com o do grep.
Depois comparamos o desempenho do índice de árvore de sufixo com o de array de sufixo. Medimos o tempo de indexação de arquivos com esses dois algoritmos, bem como o tamanho dos arquivos gerados. Também medimos o tempo de consultas de conjuntos de padrões com tamanhos diferentes.
Não fizemos testes rigorosos com o LZ77.


Indexação

Para cada arquivo de texto, testamos a indexação dos primeiros 5, 10, ... 35 MB deles. Medimos o tempo de indexação e o tamanho dos arquivos gerados.
Para cada arquivo, a primeira linha diz o tempo em segundos da indexação. Já a segunda diz o tamanho em bytes do arquivo gerado.


			5MB			10MB		15MB		20MB		25MB		30MB		35MB

english		24.210000	49.690000	75.990000	102.420000	129.080000	157.760000	186.540000
			106145741	231637987	355041296	491003716	620815992	737319871	858603287

dna			21.960000	45.070000	68.960000	93.180000	119.100000	144.990000	201.400000
			108008118	226149970	347878776	470663447	597206517	722902754	849338137

sources		18.270000	37.620000	59.640000	80.390000	101.650000	119.970000	149.680000
			107788837	226834624	347229033	469087606	595764669	719374196	842817283

proteins	24.600000	50.380000	80.660000	108.170000	141.550000	167.810000	204.140000
			103484312	229733955	353999112	475259394	599355976	709242764	817903353

dblp.xml	18.550000	38.280000	59.090000	78.110000	99.320000	117.810000	140.230000
			95245799	200100552	308498854	416999777	528990260	641102313	752168944


Apesar dos tempos de indexação variarem conforme o arquivo de texto, dá pra notar um comportamento quase linear em todos eles, em função do tamanho do texto original: os arquivos de 5MB demoram uma média de 4.3 segundos por MB, enquanto os maiores chegam a uma média de 5.03 segundos por MB.
Já sobre o tamanho do índice gerado, quase todos os arquivos têm resultados semelhantes, exceto pelo dblp.xml, cujos arquivos gerados são ligeiramente menores do que os outros. Os arquivos gerados também apresentam um comportamento linear em função do tamanho do arquivo original. Os arquivos de 5MB geram índices de tamanho, em média, 20.8 vezes maior. Já os de 35MB geram arquivos em média 23.55 vezes maiores.

Para comparar a indexação com os algoritmos de árvore de sufixo e array de sufixo, indexamos o primeiro MB de cada um dos 5 arquivos de texto, medindo o tempo e o tamanho do arquivo gerado.

dna		english		proteins	sources		dblp.xml




Consulta

proteins
Indexamos os primeiros 35MB desse arquivo e submetemos consultas de 100 padrões cada, todos de mesmo tamanho. Esses padrões eram gerados aleatoriamente com as letras "ACDEFGHIKLMNPQRSTVWXY", presentes no texto. Como tamanho dos padrões, testamos 2, 4, 6, 8 e 10, além de testar um conjunto sem padrões para comparação. Também executamos consultas com o grep. Os resultados foram estes:


		nenhum padrão	tamanho 2	tamanho 4	tamanho 6	tamanho 8	tamanho 10

ipmt	121.360000		128.043333	121.443333	119.613333	121.243333	121.323333
grep	0.023333		0.040000	0.210000	0.263333	0.233333	0.270000


dna
Fizemos a mesma coisa que o proteins, só trocando o conjunto de letras por "ACGT". Os resultados foram estes:


		nenhum padrão	tamanho 2	tamanho 4	tamanho 6	tamanho 8	tamanho 10

ipmt	136.146667		248.160000	140.176667	135.796667	135.863333	134.696667
grep	0.023333		0.026667	0.026667	0.036667	0.050000	0.093333


Para o nosso programa, os conjuntos de padrões de tamanho 2 têm consultas bem mais demoradas. Isto provavelmente é por causa da alta probabilidade de ocorrência de padrões de dois caracteres, especialmente no DNA, por seu alfabeto bem restrito. O tempo extra deve ser para identificar e contar as linhas com ocorrências. Porém, tal efeito não ocorre no grep, por ser uma implementação mais eficiente.
Fora isso, não há nenhuma relação aparente entre o tamanho dos padrões e o tempo de busca, mesmo descontando o tempo do conjunto vazio. Inclusive, algumas consultas parecem levar ligeiramente menos tempo do que se não houvesse padrão nenhum. O tempo da consulta no grep, por outro lado, parece crescer em função do tamanho dos padrões, como era de se esperar. Cada consulta no nosso programa foi repetida três vezes para se tirar uma média, mas parece que isso não foi suficiente para se obter resultados robustos.

english
Para reduzir o efeito da quantidade de ocorrências do conjunto de padrões no texto, realizamos testes com um conjunto gerado da seguinte maneira: aleatoriamente foram escolhidas 20 cadeias de mesmo tamanho do próprio texto de entrada, nenhuma com espaços ou quebras de linha.


		nenhum padrão	tamanho 2	tamanho 4	tamanho 6	tamanho 8

ipmt	121.790000		130.266667	123.966667	123.916667	123.873333
grep	0.046667		0.086667	0.186667	0.180000	0.183333


A consulta do conjunto de padrões de tamanho 2 ainda parece demorar mais tempo com nosso programa. Possivelmente, a forma como os conjuntos foram escolhidos não evita que este possua mais ocorrências que os outros. Também parece que as outras consultas gastam cada vez menos tempo à medida que o tamanho dos padrões aumenta, possivelmente por causa da diminuição gradual do número do ocorrências.

Para comparar , p, 


Conclusões

Sobre a indexação, os tempos de execução aparentam se comportar linearmente em função do tamanho do arquivo indexado, além de depender do tipo de texto. Portanto, acreditamos ter implementado corretamente os algoritmos Ukkonen e LZ78, embora ainda hajam aspectos de baixo nível que possam ser otimizados.
Os tamanhos dos arquivos gerados, diferentemente dos tempos de execução, não diferem muito entre si de um tipo de texto para outro. Com exceção do dblp.xml, eles aparentam depender apenas do tamanho do arquivo. Esperávamos que as propriedades do texto influenciassem o tamanho do índice gerado. Mas parece que o armazenamento da árvore de sufixo domina o tamanho do arquivo, considerando que ela não é comprimida, apenas codificada de uma forma que consideramos eficiente. Também devemos levar em conta que, dada a forma como foi implementada, levando em conta o caractere nulo, a árvore possui no mínimo N nós, onde N é o tamanho do texto indexado. Portanto, ela não é tão econômica em espaço quanto possível na teoria.

Sobre as consultas, para índices de arquivos muito grandes, como 35MB, o tempo para leitura do arquivo de índice, construção da árvore de sufixo e descompressão do texto parece enorme em comparação ao tempo gasto com as consultas propriamente ditas. E além de encontrar as ocorrências, parece que determinar e contar as linhas às quais elas pertencem também gasta um tempo considerável. Para que essa ferramenta fosse eficiente o bastante para ser útil, seria necessário utilizar o índice sem carregá-lo por inteiro na memória, bem como mostrar as linhas com ocorrências de forma mais eficiente.


Tanto a indexação como as consultas usando array de sufixo como índice mostraram um desempenho inferior ao da árvore de sufixo. 
Nós focamos na implementação neste último, e não utilizamos as técnicas mais eficientes com o array de sufixo.

